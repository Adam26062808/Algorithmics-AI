{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c928ee2",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning & Data Preprocessing\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lesson, you will understand:\n",
    "- Core machine learning concepts and applications\n",
    "- Different types of machine learning\n",
    "- Basic ML workflow with Scikit-Learn\n",
    "- Model performance evaluation and confusion matrices\n",
    "- Data preprocessing techniques\n",
    "- Real-world ML project implementation\n",
    "\n",
    "## üìö Topics Covered\n",
    "1. **Introduction to Machine Learning**\n",
    "2. **What is Data Preprocessing?**\n",
    "3. **Project: Adult Income Classification (Part 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö All libraries imported successfully!\")\n",
    "print(\"üîß Environment configured for machine learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d85ef",
   "metadata": {},
   "source": [
    "# ü§ñ PART 1: Introduction to Machine Learning\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Algorithm**: A set of rules or instructions for solving a problem\n",
    "- **Model**: A trained algorithm that can make predictions on new data\n",
    "- **Training**: The process of teaching an algorithm using historical data\n",
    "- **Prediction**: Using a trained model to make decisions on new, unseen data\n",
    "\n",
    "### Real-World Applications:\n",
    "- üé¨ **Netflix**: Movie recommendations based on viewing history\n",
    "- üõí **Amazon**: Product recommendations and fraud detection\n",
    "- üöó **Tesla**: Self-driving cars using computer vision\n",
    "- üè• **Healthcare**: Medical diagnosis and drug discovery\n",
    "- üìß **Gmail**: Spam email detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a80901",
   "metadata": {},
   "source": [
    "## Types of Machine Learning\n",
    "\n",
    "### 1. üéØ Supervised Learning\n",
    "- **Definition**: Learning with labeled examples (input-output pairs)\n",
    "- **Goal**: Predict outcomes for new data\n",
    "- **Examples**: \n",
    "  - Predicting house prices (regression)\n",
    "  - Email spam detection (classification)\n",
    "  - Medical diagnosis (classification)\n",
    "\n",
    "### 2. üîç Unsupervised Learning  \n",
    "- **Definition**: Finding patterns in data without labels\n",
    "- **Goal**: Discover hidden structure in data\n",
    "- **Examples**:\n",
    "  - Customer segmentation (clustering)\n",
    "  - Market basket analysis (association rules)\n",
    "  - Dimensionality reduction (PCA)\n",
    "\n",
    "### 3. üéÆ Reinforcement Learning\n",
    "- **Definition**: Learning through trial and error with rewards/penalties\n",
    "- **Goal**: Learn optimal actions in an environment\n",
    "- **Examples**:\n",
    "  - Game playing (Chess, Go)\n",
    "  - Robot navigation\n",
    "  - Trading algorithms\n",
    "\n",
    "### Focus of This Course: **Supervised Learning** üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f70a2",
   "metadata": {},
   "source": [
    "## Basic ML Workflow with Scikit-Learn\n",
    "\n",
    "The typical machine learning workflow consists of these steps:\n",
    "\n",
    "### 1. üìä **Data Collection & Loading**\n",
    "   - Gather relevant data for your problem\n",
    "   - Load data into pandas DataFrame\n",
    "\n",
    "### 2. üîç **Exploratory Data Analysis (EDA)**\n",
    "   - Understand data structure and patterns\n",
    "   - Identify missing values, outliers, distributions\n",
    "\n",
    "### 3. üõ†Ô∏è **Data Preprocessing**\n",
    "   - Clean and prepare data for modeling\n",
    "   - Handle missing values, scale features, encode categories\n",
    "\n",
    "### 4. üéØ **Model Selection & Training**\n",
    "   - Choose appropriate algorithm\n",
    "   - Split data into train/test sets\n",
    "   - Train model on training data\n",
    "\n",
    "### 5. üìà **Model Evaluation**\n",
    "   - Test model performance on unseen data\n",
    "   - Use metrics like accuracy, precision, recall\n",
    "\n",
    "### 6. üöÄ **Model Deployment**\n",
    "   - Put model into production\n",
    "   - Monitor and maintain performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Example: Basic ML Workflow\n",
    "print(\"üîß BASIC ML WORKFLOW EXAMPLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Create sample data\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=42)\n",
    "\n",
    "print(f\"üìä Dataset created: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"üéØ Target classes: {np.unique(y)}\")\n",
    "\n",
    "# Step 2: Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"‚úÇÔ∏è Data split: {len(X_train)} training, {len(X_test)} testing samples\")\n",
    "\n",
    "# Step 3: Train model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"ü§ñ Model trained successfully!\")\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"üìà Model accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Basic workflow complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33568f9",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "#### 1. üéØ **Accuracy**\n",
    "- **Definition**: Percentage of correct predictions\n",
    "- **Formula**: (Correct Predictions) / (Total Predictions)\n",
    "- **When to use**: Balanced datasets\n",
    "\n",
    "#### 2. üìä **Confusion Matrix**\n",
    "- **Definition**: Table showing actual vs predicted classifications\n",
    "- **Components**:\n",
    "  - **True Positives (TP)**: Correctly predicted positive cases\n",
    "  - **True Negatives (TN)**: Correctly predicted negative cases  \n",
    "  - **False Positives (FP)**: Incorrectly predicted as positive (Type I error)\n",
    "  - **False Negatives (FN)**: Incorrectly predicted as negative (Type II error)\n",
    "\n",
    "#### 3. üîç **Precision**\n",
    "- **Definition**: Of all positive predictions, how many were correct?\n",
    "- **Formula**: TP / (TP + FP)\n",
    "\n",
    "#### 4. üìà **Recall (Sensitivity)**\n",
    "- **Definition**: Of all actual positives, how many did we find?\n",
    "- **Formula**: TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ee9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Confusion Matrix\n",
    "print(\"üìä CONFUSION MATRIX EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create and visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1'], \n",
    "            yticklabels=['Class 0', 'Class 1'])\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics manually\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nüìà Detailed Metrics:\")\n",
    "print(f\"True Negatives:  {tn}\")\n",
    "print(f\"False Positives: {fp}\")  \n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives:  {tp}\")\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\nüéØ Performance Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ea895",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è PART 2: What is Data Preprocessing?\n",
    "\n",
    "## Why Preprocessing Matters\n",
    "\n",
    "Raw data is rarely ready for machine learning algorithms. Data preprocessing is crucial because:\n",
    "\n",
    "### üö´ **Common Data Problems:**\n",
    "- **Missing values**: Gaps in data that need to be filled\n",
    "- **Different scales**: Features with vastly different ranges (e.g., age vs income)\n",
    "- **Categorical data**: Text labels that need numeric encoding\n",
    "- **Outliers**: Extreme values that can skew results\n",
    "- **Irrelevant features**: Noise that hurts model performance\n",
    "\n",
    "### ‚úÖ **Benefits of Preprocessing:**\n",
    "- **Improved accuracy**: Clean data leads to better predictions\n",
    "- **Faster training**: Optimized data trains models quicker\n",
    "- **Algorithm compatibility**: Makes data suitable for ML algorithms\n",
    "- **Better convergence**: Helps optimization algorithms work properly\n",
    "\n",
    "### üéØ **The Golden Rule:**\n",
    "> **\"Garbage in, garbage out\"** - Quality preprocessing is essential for quality results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b145a4",
   "metadata": {},
   "source": [
    "## Key Preprocessing Techniques\n",
    "\n",
    "### 1. üìè **MinMaxScaler**\n",
    "- **Purpose**: Scale features to a fixed range (usually 0-1)\n",
    "- **Formula**: (value - min) / (max - min)\n",
    "- **When to use**: When features have different scales\n",
    "- **Pros**: Preserves relationships, bounded output\n",
    "- **Cons**: Sensitive to outliers\n",
    "\n",
    "### 2. üè∑Ô∏è **OneHotEncoder**  \n",
    "- **Purpose**: Convert categorical variables to binary columns\n",
    "- **Example**: ['Red', 'Blue', 'Green'] ‚Üí [1,0,0], [0,1,0], [0,0,1]\n",
    "- **When to use**: Nominal categories (no order)\n",
    "- **Pros**: No artificial ordering imposed\n",
    "- **Cons**: Can create many columns (curse of dimensionality)\n",
    "\n",
    "### 3. üî¢ **LabelEncoder**\n",
    "- **Purpose**: Convert categorical variables to integers\n",
    "- **Example**: ['Small', 'Medium', 'Large'] ‚Üí [0, 1, 2]\n",
    "- **When to use**: Ordinal categories (natural order)\n",
    "- **Pros**: Compact representation\n",
    "- **Cons**: May imply false ordering for nominal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Preprocessing Techniques\n",
    "print(\"üõ†Ô∏è PREPROCESSING DEMONSTRATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample data with different types\n",
    "np.random.seed(42)\n",
    "sample_data = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 80, 100),\n",
    "    'income': np.random.normal(50000, 20000, 100),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 100),\n",
    "    'city': np.random.choice(['New York', 'London', 'Tokyo', 'Sydney'], 100)\n",
    "})\n",
    "\n",
    "print(\"üìä Original Sample Data:\")\n",
    "print(sample_data.head())\n",
    "print(f\"\\nData types:\\n{sample_data.dtypes}\")\n",
    "print(f\"\\nData shape: {sample_data.shape}\")\n",
    "\n",
    "# 1. MinMaxScaler Example\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"üìè MINMAXSCALER EXAMPLE\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "numerical_cols = ['age', 'income']\n",
    "scaled_data = scaler.fit_transform(sample_data[numerical_cols])\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=numerical_cols)\n",
    "\n",
    "print(\"Before scaling:\")\n",
    "print(sample_data[numerical_cols].describe())\n",
    "print(\"\\nAfter MinMax scaling (0-1 range):\")\n",
    "print(scaled_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. OneHotEncoder Example\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"üè∑Ô∏è ONEHOTENCODER EXAMPLE\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# OneHot encode city (nominal categorical)\n",
    "ohe = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "city_encoded = ohe.fit_transform(sample_data[['city']])\n",
    "city_columns = [f\"city_{cat}\" for cat in ohe.categories_[0][1:]]  # Skip first category (dropped)\n",
    "city_df = pd.DataFrame(city_encoded, columns=city_columns)\n",
    "\n",
    "print(\"Original city values (first 10):\")\n",
    "print(sample_data['city'].head(10).tolist())\n",
    "print(f\"\\nUnique cities: {sample_data['city'].unique()}\")\n",
    "print(f\"\\nOneHot encoded columns: {city_columns}\")\n",
    "print(\"\\nEncoded representation (first 10 rows):\")\n",
    "print(city_df.head(10))\n",
    "\n",
    "# 3. LabelEncoder Example  \n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"üî¢ LABELENCODER EXAMPLE\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Label encode education (ordinal categorical)\n",
    "le = LabelEncoder()\n",
    "education_encoded = le.fit_transform(sample_data['education'])\n",
    "\n",
    "print(\"Original education values (first 10):\")\n",
    "print(sample_data['education'].head(10).tolist())\n",
    "print(f\"\\nUnique education levels: {sample_data['education'].unique()}\")\n",
    "print(f\"Label mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "print(f\"\\nEncoded education values (first 10): {education_encoded[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d02cb1",
   "metadata": {},
   "source": [
    "## Scikit-Learn Pipelines & Avoiding Data Leakage\n",
    "\n",
    "### üîß **What are Pipelines?**\n",
    "Pipelines are a way to chain preprocessing steps and models together, ensuring:\n",
    "- **Reproducibility**: Same steps applied consistently\n",
    "- **Code cleanliness**: Organized and readable workflow\n",
    "- **Parameter tuning**: Easy to optimize entire pipeline\n",
    "- **Deployment**: Simple to put into production\n",
    "\n",
    "### ‚ö†Ô∏è **Data Leakage Prevention**\n",
    "**Data leakage** occurs when information from the future or test set \"leaks\" into training:\n",
    "\n",
    "#### **Common Leakage Sources:**\n",
    "1. **Preprocessing before splitting**: Scaling using entire dataset statistics\n",
    "2. **Target leakage**: Features that wouldn't be available at prediction time\n",
    "3. **Temporal leakage**: Using future information to predict the past\n",
    "\n",
    "#### **Prevention Strategy:**\n",
    "- **‚úÖ Fit preprocessing on training data only**\n",
    "- **‚úÖ Transform both training and test data with training statistics**\n",
    "- **‚ùå Never fit preprocessing on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Example: Proper Way to Avoid Data Leakage\n",
    "print(\"üîß PIPELINE EXAMPLE - AVOIDING DATA LEAKAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create sample data\n",
    "X_sample = sample_data[['age', 'income', 'education', 'city']].copy()\n",
    "y_sample = np.random.choice([0, 1], size=len(X_sample))\n",
    "\n",
    "# Split data FIRST (before any preprocessing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"üìä Data split: {len(X_train)} training, {len(X_test)} testing samples\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "# Define column types\n",
    "numeric_features = ['age', 'income']\n",
    "categorical_features = ['education', 'city']\n",
    "\n",
    "# Create transformers\n",
    "numeric_transformer = MinMaxScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create full pipeline with model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "print(\"üîß Pipeline created with preprocessing + model\")\n",
    "\n",
    "# Fit pipeline (preprocessing + model together)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"‚úÖ Pipeline fitted on training data only\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_pipeline = pipeline.predict(X_test)\n",
    "accuracy_pipeline = accuracy_score(y_test, y_pred_pipeline)\n",
    "\n",
    "print(f\"üìà Pipeline accuracy: {accuracy_pipeline:.3f}\")\n",
    "print(\"\\nüéØ Benefits achieved:\")\n",
    "print(\"   ‚úÖ No data leakage\")\n",
    "print(\"   ‚úÖ Reproducible preprocessing\") \n",
    "print(\"   ‚úÖ Easy deployment\")\n",
    "print(\"   ‚úÖ Clean, organized code\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
